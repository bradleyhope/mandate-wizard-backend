#!/usr/bin/env python3
"""
Mandate Wizard - Production Web Application
HybridRAG Query Engine with Pinecone + Neo4j
"""

from flask import Flask, render_template, request, jsonify, session, Response, stream_with_context
import os
import secrets
from hybridrag_engine_pinecone import HybridRAGEnginePinecone
import json
from executive_deep_dive import ExecutiveDeepDive

app = Flask(__name__)
app.secret_key = secrets.token_hex(32)

# Conversation memory: session_id -> list of (question, answer, context)
conversation_memory = {}

# Initialize HybridRAG engine
print("Initializing Mandate Wizard HybridRAG Engine...")

# Database credentials (from starter package)
PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY', 'pcsk_2kvuLD_NLVH2XehCeitZUi3VCUJVkeH3KaceWniEE59Nh8f7GucxBNJDdg2eedfTaeYiD1')
PINECONE_INDEX_NAME = 'netflix-mandate-wizard'

NEO4J_URI = os.environ.get('NEO4J_URI', 'neo4j+s://0dd3462a.databases.neo4j.io')
NEO4J_USER = os.environ.get('NEO4J_USER', 'neo4j')
NEO4J_PASSWORD = os.environ.get('NEO4J_PASSWORD', 'cH-Jo3f9mcbbOr9ov-x22V7AQB3kOxxV42JJR55ZbMg')

# Initialize engine
engine = HybridRAGEnginePinecone(
    pinecone_api_key=PINECONE_API_KEY,
    pinecone_index_name=PINECONE_INDEX_NAME,
    neo4j_uri=NEO4J_URI,
    neo4j_user=NEO4J_USER,
    neo4j_password=NEO4J_PASSWORD
)

# Initialize executive deep dive
deep_dive = ExecutiveDeepDive(
    neo4j_uri=NEO4J_URI,
    neo4j_user=NEO4J_USER,
    neo4j_password=NEO4J_PASSWORD
)

print("âœ“ Mandate Wizard ready!")

@app.route('/')
def index():
    """Serve the main interface"""
    return render_template('index.html')

@app.route('/ask', methods=['POST'])
def ask():
    """Handle question and return answer with conversation memory"""
    data = request.json
    question = data.get('question', '')
    session_id = data.get('session_id', 'default')
    
    if not question:
        return jsonify({'error': 'No question provided'}), 400
    
    try:
        # Get conversation history
        if session_id not in conversation_memory:
            conversation_memory[session_id] = []
        
        history = conversation_memory[session_id][-20:]  # Last 20 exchanges for deep context
        
        # Query the HybridRAG engine with history
        result = engine.query(question, conversation_history=history)
        
        # Store in conversation memory
        conversation_memory[session_id].append({
            'question': question,
            'answer': result['answer'],
            'intent': result.get('intent', 'HYBRID'),
            'context': result.get('context', {})
        })
        
        # Limit memory to last 30 exchanges per session (keep 20 for context + 10 buffer)
        if len(conversation_memory[session_id]) > 30:
            conversation_memory[session_id] = conversation_memory[session_id][-30:]
        
        return jsonify({
            'answer': result['answer'],
            'followups': result.get('followups', []),
            'resources': result.get('resources', []),
            'intent': result.get('intent', 'HYBRID'),
            'session_id': session_id,
            'success': True
        })
        
    except Exception as e:
        print(f"Error processing query: {e}")
        return jsonify({
            'error': str(e),
            'success': False
        }), 500

@app.route('/ask_stream', methods=['POST'])
def ask_stream():
    """Handle question and return streaming answer using Server-Sent Events"""
    data = request.json
    question = data.get('question', '')
    session_id = data.get('session_id', 'default')
    
    if not question:
        return jsonify({'error': 'No question provided'}), 400
    
    def generate():
        try:
            # Get conversation history
            if session_id not in conversation_memory:
                conversation_memory[session_id] = []
            
            history = conversation_memory[session_id][-20:]  # Last 20 exchanges for deep context
            
            # Stream the query results
            full_answer = ""
            result_data = {'followups': [], 'resources': []}
            
            for event in engine.query_with_streaming(question, conversation_history=history):
                if event['type'] == 'chunk':
                    full_answer += event['content']
                elif event['type'] == 'followups':
                    result_data['followups'] = event['data']
                elif event['type'] == 'resources':
                    result_data['resources'] = event['data']
                
                # Send event to client
                yield f"data: {json.dumps(event)}\n\n"
            
            # Store in conversation memory
            conversation_memory[session_id].append({
                'question': question,
                'answer': full_answer,
                'intent': 'STREAMING',
                'context': {}
            })
            
            # Limit memory to last 30 exchanges per session
            if len(conversation_memory[session_id]) > 30:
                conversation_memory[session_id] = conversation_memory[session_id][-30:]
            
        except Exception as e:
            print(f"Error in streaming query: {e}")
            error_event = {'type': 'error', 'message': str(e)}
            yield f"data: {json.dumps(error_event)}\n\n"
    
    return Response(stream_with_context(generate()), mimetype='text/event-stream')

@app.route('/stats', methods=['GET'])
def stats():
    """Return database statistics"""
    try:
        return jsonify({
            'persons': len(engine.persons),
            'mandates': len(engine.mandates),
            'projects': len(engine.projects),
            'regions': len(engine.persons_by_region),
            'genres': len(engine.persons_by_genre),
            'formats': len(engine.persons_by_format),
            'success': True
        })
    except Exception as e:
        return jsonify({
            'error': str(e),
            'success': False
        }), 500

@app.route('/executives', methods=['GET'])
def list_executives():
    """List all executives"""
    try:
        executives = deep_dive.list_all_executives()
        return jsonify({
            'executives': executives,
            'count': len(executives),
            'success': True
        })
    except Exception as e:
        return jsonify({
            'error': str(e),
            'success': False
        }), 500

@app.route('/executive/<exec_name>', methods=['GET'])
def executive_profile(exec_name):
    """Get full executive profile (API or HTML)"""
    try:
        # Check if request wants JSON (API) or HTML (browser)
        if request.accept_mimetypes.best == 'application/json' or request.args.get('format') == 'json':
            # Return JSON for API calls
            profile = deep_dive.generate_profile(exec_name)
            
            if 'error' in profile:
                return jsonify({
                    'error': profile['error'],
                    'success': False
                }), 404
            
            return jsonify({
                'profile': profile,
                'success': True
            })
        else:
            # Return HTML page for browser
            return render_template('executive_profile.html')
    except Exception as e:
        return jsonify({
            'error': str(e),
            'success': False
        }), 500

if __name__ == '__main__':
    print("\n" + "="*70)
    print("ðŸŽ¬ MANDATE WIZARD - Web Application")
    print("="*70)
    print(f"âœ“ Connected to Pinecone vector database")
    print(f"âœ“ Connected to Neo4j graph database")
    print(f"âœ“ Loaded {len(engine.persons)} executives from Neo4j")
    print(f"âœ“ HybridRAG engine ready")
    print("="*70)
    print("\nðŸš€ Starting server on http://0.0.0.0:5000")
    print("\n")
    
    app.run(host='0.0.0.0', port=5000, debug=False)




# ============================================
# LANGGRAPH PATHWAY ENDPOINT
# ============================================

from pathway_graph import MandateWizardPathway

# Initialize pathway system
print("Initializing LangGraph pathway system...")
pathway = MandateWizardPathway(rag_engine=engine)
print("âœ“ LangGraph pathway ready!")

@app.route('/ask_pathway', methods=['POST'])
def ask_pathway():
    """
    Handle question using LangGraph pathway system
    Returns persona-adapted answer with pathway navigation
    """
    data = request.json
    question = data.get('question', '')
    user_id = data.get('user_id', 'anonymous')
    session_id = data.get('session_id', 'default')
    
    if not question:
        return jsonify({'error': 'No question provided'}), 400
    
    try:
        # Run through LangGraph pathway
        result = pathway.run(query=question, user_id=user_id)
        
        # Store in conversation memory
        if session_id not in conversation_memory:
            conversation_memory[session_id] = []
        
        conversation_memory[session_id].append({
            'question': question,
            'answer': result['answer'],
            'intent': 'PATHWAY',
            'context': {
                'user_profile': result['user_profile'],
                'layers_visited': result['current_layer'],
                'executive_name': result.get('executive_name', '')
            }
        })
        
        # Return enriched result
        return jsonify({
            'answer': result['answer'],
            'follow_ups': result['follow_ups'],
            'confidence_score': result['confidence_score'],
            'user_profile': result['user_profile'],
            'response_strategy': result['response_strategy'],
            'layers_visited': result['current_layer'],
            'layers_needed': result['layers_needed'],
            'executive_name': result.get('executive_name', ''),
            'sources': result.get('sources', []),
            'success': True
        })
        
    except Exception as e:
        print(f"Error in pathway query: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({
            'error': str(e),
            'success': False
        }), 500

@app.route('/ask_pathway_stream', methods=['POST'])
def ask_pathway_stream():
    """
    Handle question using LangGraph pathway system with streaming
    Returns persona-adapted answer with real-time updates
    """
    data = request.json
    question = data.get('question', '')
    user_id = data.get('user_id', 'anonymous')
    session_id = data.get('session_id', 'default')
    
    if not question:
        return jsonify({'error': 'No question provided'}), 400
    
    def generate():
        try:
            # Send initial status
            yield f"data: {json.dumps({'type': 'status', 'message': 'Detecting persona...'})}\n\n"
            
            # Run through LangGraph pathway
            result = pathway.run(query=question, user_id=user_id)
            
            # Send user profile
            yield f"data: {json.dumps({'type': 'profile', 'data': result['user_profile']})}\n\n"
            
            # Send layers visited
            yield f"data: {json.dumps({'type': 'layers', 'data': {'visited': result['current_layer'], 'needed': result['layers_needed']}})}\n\n"
            
            # Send answer in chunks (simulate streaming)
            answer = result['answer']
            chunk_size = 50
            for i in range(0, len(answer), chunk_size):
                chunk = answer[i:i+chunk_size]
                yield f"data: {json.dumps({'type': 'chunk', 'content': chunk})}\n\n"
            
            # Send follow-ups
            yield f"data: {json.dumps({'type': 'followups', 'data': result['follow_ups']})}\n\n"
            
            # Send sources
            if result.get('sources'):
                yield f"data: {json.dumps({'type': 'sources', 'data': result['sources']})}\n\n"
            
            # Send completion
            yield f"data: {json.dumps({'type': 'complete', 'confidence': result['confidence_score']})}\n\n"
            
            # Store in conversation memory
            if session_id not in conversation_memory:
                conversation_memory[session_id] = []
            
            conversation_memory[session_id].append({
                'question': question,
                'answer': result['answer'],
                'intent': 'PATHWAY_STREAM',
                'context': {
                    'user_profile': result['user_profile'],
                    'layers_visited': result['current_layer']
                }
            })
            
        except Exception as e:
            print(f"Error in pathway streaming: {e}")
            import traceback
            traceback.print_exc()
            error_event = {'type': 'error', 'message': str(e)}
            yield f"data: {json.dumps(error_event)}\n\n"
    
    return Response(stream_with_context(generate()), mimetype='text/event-stream')

